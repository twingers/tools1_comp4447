{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text normalization cont.\n",
    "Last time we finished with some text normalization activities like stemming and normalization(removing [inflectional](https://en.wikipedia.org/wiki/Inflection) affixes**(ed, ing, ize, s, de)**).\n",
    "\n",
    "Note that \n",
    "\n",
    "- stemming can result in a word not in the dictionary.\n",
    "- Lemmatization ensures word is in dictionary.\n",
    "- stemming is a fast process compared to lemmatization.\n",
    "\n",
    "\n",
    "We can use use both the techniques to further reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'muse'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "word= 'muses'\n",
    "ps.stem(word)\n",
    "!pip install nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mus\n",
      "mu\n"
     ]
    }
   ],
   "source": [
    "wn_lm= WordNetLemmatizer()\n",
    "lm_word = wn_lm.lemmatize(word)\n",
    "print(lm_word)\n",
    "print(ps.stem(lm_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Expanding contractions\n",
    "\n",
    "This activity invloves replacing contractions with full words like\n",
    "- can't with cannot.\n",
    "- Should've with should have\n",
    "- Weren't were not\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Any suggestion how to do it? What module and function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "contraction_patterns=[(r'can\\'t', 'cannot'),\n",
    "                    (r'haven\\'t', 'have not'),\n",
    "                    (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "                    (r'(\\w+)\\'re', '\\g<1> are')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class contraction_replacer(object):\n",
    "    def __init__(self, contraction_patterns):        \n",
    "        # store compiled regex object\n",
    "        self._contraction_regexes = [(re.compile(p), replaced_text) for p, replaced_text in contraction_patterns]\n",
    "        \n",
    "    def __do_contraction_normalization(self, text):\n",
    "        for contraction_regex, replaced_text in self._contraction_regexes:\n",
    "            text = contraction_regex.sub(replaced_text, text)\n",
    "        return text     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Let's use it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sample_contraction_replacer = contraction_replacer(contraction_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We will do this work'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_contraction_replacer.do_contraction_normalization(\"We'll do this work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We', 'will', 'do', 'this', 'work']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing contraction and tokenize\n",
    "nltk.tokenize.word_tokenize(sample_contraction_replacer.do_contraction_normalization(\"We'll do this work\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Removing repeated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class repeat_replacer(object):\n",
    "    def __init__(self, repeat_patterns, sub_pattern):       \n",
    "        \n",
    "        # store compiled regex object\n",
    "        self._repeat_regexes = re.compile(repeat_patterns)\n",
    "        self._sub_pattern = sub_pattern\n",
    "    def do_repeat_normalization(self, word):\n",
    "        compressed_word = self._repeat_regexes.sub(self._sub_pattern, word)\n",
    "        if compressed_word != word:\n",
    "            compressed_word = self.do_repeat_normalization(compressed_word)\n",
    "            \n",
    "        \n",
    "        return compressed_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Notice how backreferences(\\1, \\2, \\3) are used  I looove loove love \n",
    "sample_repeat_replacer = repeat_replacer(r'(\\w*)(\\w)\\2(\\w*)', r'\\1\\2\\3' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('oh', 'love')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_repeat_replacer.do_repeat_normalization('ooooh'), sample_repeat_replacer.do_repeat_normalization('loooove')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What happens when word has repeating character!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shep'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_repeat_replacer.do_repeat_normalization('sheep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "class repeat_replacer(object):\n",
    "    def __init__(self, repeat_patterns, sub_pattern):\n",
    "        \n",
    "        \n",
    "        # store compiled regex object\n",
    "        self._repeat_regexes = re.compile(repeat_patterns)\n",
    "        self._sub_pattern = sub_pattern\n",
    "    def do_repeat_normalization(self, word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "        compressed_word = self._repeat_regexes.sub(self._sub_pattern, word)\n",
    "        if compressed_word != word:\n",
    "            #print('iside if')\n",
    "            compressed_word = self.do_repeat_normalization(compressed_word)\n",
    "        return compressed_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "repeat_replacer_inst = repeat_replacer(r'(\\w*)(\\w)\\2(\\w*)', r'\\1\\2\\3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sheep'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeat_replacer_inst.do_repeat_normalization('sheep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spelling correction with Enchant\n",
    "\n",
    "Go to \n",
    "http://www.abisource.com/projects/enchant/ \n",
    "to learn more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyenchant\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/54/04d88a59efa33fefb88133ceb638cdf754319030c28aadc5a379d82140ed/pyenchant-2.0.0.tar.gz (64kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 2.4MB/s ta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: pyenchant\n",
      "  Running setup.py install for pyenchant ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed pyenchant-2.0.0\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyenchant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's build a spell checker class, We need\n",
    "- a spellchecking library like enchant. We just installed it\n",
    "- and a dictionary for it to use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# aspell demo at command prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Let' see how enchant works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('en_US', <Enchant: Myspell Provider>),\n",
       " ('en', <Enchant: Aspell Provider>),\n",
       " ('en_CA', <Enchant: Aspell Provider>),\n",
       " ('en_GB', <Enchant: Aspell Provider>)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import enchant\n",
    "enchant.list_dicts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_int = enchant.Dict('en')\n",
    "dict_int.check('love'), dict_int.check('lov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scion',\n",
       " 'skien',\n",
       " 'sci en',\n",
       " 'sci-en',\n",
       " 'science',\n",
       " 'scenic',\n",
       " 'scene',\n",
       " 'menisci',\n",
       " 'Siena',\n",
       " 'Lucien']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_int.suggest('scien')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How edit distance works\n",
    "\n",
    "*minimum number of character changes to transform one word into another.*\n",
    "\n",
    "See wiki for details\n",
    "https://en.wikipedia.org/wiki/Edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics import edit_distance\n",
    "edit_distance('sciena', 'science')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Let's write the class for performing spell correction\n",
    "- import enchant and initialize a dictionary(will use opensource aspell http://aspell.net/) for it to use\n",
    "- import edit_distance  from nltk.metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "from nltk.metrics import edit_distance\n",
    "import numpy as np\n",
    "\n",
    "class spell_checker(object):\n",
    "    def __init__(self, dict_name='en_US', max_edit_dist=3):\n",
    "        self._dict= enchant.Dict(dict_name)\n",
    "        self._max_edit_dist = max_edit_dist\n",
    "    def _word_with_min_dist(self, word, suggestions):\n",
    "        print(suggestions)\n",
    "        #min_edit_distance = np.inf\n",
    "        corrected_word = word\n",
    "        for sug in [suggestions[0]]:\n",
    "            distance = edit_distance(word, sug)\n",
    "            #print(distance)\n",
    "            if distance < self._max_edit_dist:\n",
    "                print(distance, sug)\n",
    "                min_edit_distance = distance\n",
    "                corrected_word = sug\n",
    "        return corrected_word        \n",
    "                \n",
    "                \n",
    "        \n",
    "    def check_spell(self, word):\n",
    "        if self._dict.check(word):\n",
    "            return word\n",
    "        # the the words with minimum distance\n",
    "        return self._word_with_min_dist(word, self._dict.suggest(word))\n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "spell_check_int = spell_checker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jukebox', 'juke', 'cookbook', 'kickback']\n",
      "1 jukebox\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'jukebox'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell_check_int.check_spell('jukeboc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Use right dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'theater'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_spell_ckeck_inst = spell_checker('en_US')\n",
    "us_spell_ckeck_inst.check_spell('theater')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['theatre', 'heater', 'cheater', 'theta', 'that', 'eater', 'hater', 'tater', 'threader', 'beater', 'header', 'neater', 'teeter', 'Theiler']\n",
      "2 theatre\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'theatre'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br_spell_ckeck_inst = spell_checker('en_GB')\n",
    "br_spell_ckeck_inst.check_spell('theater')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Adding custom word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeplearning\n",
      "nlp\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo -e \"deeplearning\\nnlp\" > my_words.txxxt\n",
    "cat my_words.txxxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = enchant.Dict('en_US')\n",
    "d1.check('nlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2 = enchant.DictWithPWL ('en_US', 'my_words.txxxt')\n",
    "d2.check('nlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class synonynm(object):\n",
    "    def __init__(self, word_map):\n",
    "        self._map = word_map\n",
    "    def get_synonym(self, word):\n",
    "        return self._map.get(word, word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "synonynm_inst = synonynm({'bday': 'birthday', 'yolo':'you live only once'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you live only once'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonynm_inst.get_synonym('yolo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "we could have maintained a dictionary but this solution is not a extensible solution. One can maintain synonym dictionary in any format and synonym class can acts a wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "class csv_based_synonym(synonynm):\n",
    "    def __init__(self, file_name):\n",
    "        word_map = {}\n",
    "        for line in csv.reader(open(file_name)):\n",
    "            word, syn = line\n",
    "            word_map[word] = syn\n",
    "        super(csv_based_synonym ,self).__init__(word_map)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hpy, happy\n",
      "bday, birthday\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Let's create a csv file\n",
    "echo -e 'hpy, happy\\nbday, birthday' > syn.csv\n",
    "cat syn.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' happy'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_based_synonym_int = csv_based_synonym('syn.csv')\n",
    "csv_based_synonym_int.get_synonym('hpy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Replacing negation with antonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# review of WordNet(a lexical database for the English language)\n",
    "\n",
    "nltk provides an interface to WordNet synset(synonymous words that express the same concept.) lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('hike.n.01'),\n",
       " Synset('rise.n.09'),\n",
       " Synset('raise.n.01'),\n",
       " Synset('hike.v.01'),\n",
       " Synset('hike.v.02')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('hike')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hike.n.01\n",
      "a long walk usually for exercise or pleasure\n",
      "rise.n.09\n",
      "an increase in cost\n",
      "raise.n.01\n",
      "the amount a salary is increased\n",
      "hike.v.01\n",
      "increase\n",
      "hike.v.02\n",
      "walk a long way, as for pleasure or physical exercise\n"
     ]
    }
   ],
   "source": [
    "for syn in wordnet.synsets('hike'):\n",
    "    print(syn.name())\n",
    "    print(syn.definition())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Looking for lemmas and synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('science.n.01'), Synset('skill.n.02')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take first synset for science\n",
    "syn = wordnet.synsets('science')[0]\n",
    "syn\n",
    "wordnet.synsets('science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('science.n.01.science'), Lemma('science.n.01.scientific_discipline')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['science', 'scientific_discipline']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can treat lemmas as synonyms\n",
    "[l.name() for l in syn.lemmas()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Antonyms\n",
    "lemmas has antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('gladiolus.n.01'),\n",
       " Synset('glad.a.01'),\n",
       " Synset('glad.s.02'),\n",
       " Synset('glad.s.03'),\n",
       " Synset('beaming.s.01')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('glad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'showing or causing joy and pleasure; especially made happy'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn = wordnet.synsets('glad')[1]\n",
    "syn.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('sad.a.01.sad')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glad_antonyms = syn.lemmas()[0].antonyms()\n",
    "glad_antonyms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'experiencing or showing sorrow or unhappiness'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glad_antonyms[0].synset().definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Back to replacing negation with antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class antonym_replacer(object):\n",
    "    def _find_antonym(self, word, pos=None):\n",
    "        antonyms = set()\n",
    "        \n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lem in syn.lemmas():\n",
    "                for ant in lem.antonyms():\n",
    "                    antonyms.add(ant.name())\n",
    "        if len(antonyms) == 1:\n",
    "            return antonyms.pop()\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    def remove_negation(self, sent):\n",
    "        s=0\n",
    "        l=len(sent)\n",
    "        clean_words= []\n",
    "        while s < l -1:\n",
    "            possible_not_word = sent[s]\n",
    "            word = sent[s +1 ]\n",
    "            if possible_not_word == 'not':\n",
    "                ant= self._find_antonym(word)\n",
    "                print(ant)\n",
    "                if ant:\n",
    "                    clean_words.append(ant)\n",
    "                    s+=2\n",
    "            else:\n",
    "                clean_words.append(possible_not_word)\n",
    "                \n",
    "                if s==l-2:\n",
    "                    clean_words.append(word)\n",
    "                s+=1\n",
    "            \n",
    "        return clean_words    \n",
    "                                     \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', \"'s\", 'not', 'uglify', 'this', 'place']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Let's not uglify    this place\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Let's\", 'not', 'uglify', 'this', 'place']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What if we want Let's together\n",
    "\n",
    "regex= nltk.RegexpTokenizer(r'\\s+' , gaps=True)\n",
    "regex.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beautify\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Let', \"'s\", 'beautify', 'this', 'place']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antonym_replacer_inst= antonym_replacer()\n",
    "antonym_replacer_inst.remove_negation(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Side: WordNet Methods you may find useful for your work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Synstes are organised in the form of a tree using **hypernyms** and **hyponyms**\n",
    "\n",
    "**hypernyms:** abstract terms are known as hypernyms\n",
    "\n",
    "**hyponyms:** more specific terms as hyponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hike.n.01\n",
      "a long walk usually for exercise or pleasure\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Synset('walk.n.04')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn = wordnet.synsets('hike')[0]\n",
    "print(syn.name())\n",
    "print(syn.definition())\n",
    "syn.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('amble.n.01'),\n",
       " Synset('constitutional.n.01'),\n",
       " Synset('foot.n.07'),\n",
       " Synset('hike.n.01'),\n",
       " Synset('last_mile.n.01'),\n",
       " Synset('moonwalk.n.02'),\n",
       " Synset('perambulation.n.01'),\n",
       " Synset('turn.n.12'),\n",
       " Synset('walk-through.n.04'),\n",
       " Synset('walkabout.n.03')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.hypernyms()[0].hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset('entity.n.01'),\n",
       "  Synset('abstraction.n.06'),\n",
       "  Synset('psychological_feature.n.01'),\n",
       "  Synset('event.n.01'),\n",
       "  Synset('act.n.02'),\n",
       "  Synset('action.n.01'),\n",
       "  Synset('change.n.03'),\n",
       "  Synset('motion.n.06'),\n",
       "  Synset('travel.n.01'),\n",
       "  Synset('walk.n.04'),\n",
       "  Synset('hike.n.01')]]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.hypernym_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Finding similarity\n",
    "Using hypernym tree for similarity between the Synsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Wu-Palmer Similarity\n",
    "\n",
    "It is based on how similar the word senses are and realtive position of synsets in hypernym tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('faux_pas.n.01'),\n",
       " Synset('slip.n.02'),\n",
       " Synset('slip.n.03'),\n",
       " Synset('cutting.n.02'),\n",
       " Synset('slip.n.05'),\n",
       " Synset('mooring.n.01'),\n",
       " Synset('slip.n.07'),\n",
       " Synset('slickness.n.03'),\n",
       " Synset('strip.n.02'),\n",
       " Synset('slip.n.10'),\n",
       " Synset('chemise.n.01'),\n",
       " Synset('case.n.19'),\n",
       " Synset('skid.n.03'),\n",
       " Synset('slip.n.14'),\n",
       " Synset('slip.n.15'),\n",
       " Synset('steal.v.02'),\n",
       " Synset('slip.v.02'),\n",
       " Synset('skid.v.04'),\n",
       " Synset('slip.v.04'),\n",
       " Synset('slip.v.05'),\n",
       " Synset('err.v.01'),\n",
       " Synset('slip.v.07'),\n",
       " Synset('slip.v.08'),\n",
       " Synset('slip.v.09'),\n",
       " Synset('slip.v.10'),\n",
       " Synset('dislocate.v.01')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns = wordnet.synsets('slip')\n",
    "syns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8235294117647058"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.wup_similarity(syns[0], syns[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11764705882352941"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.wup_similarity(syns[0], wordnet.synsets('apple')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Above metric uses shortest path distance between the two Synsets and their common hypernym. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Synset('faux_pas.n.01'), Synset('slip.n.02'))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = syns[0]\n",
    "d2 = syns[1]\n",
    "d1,d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1.shortest_path_distance(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('blunder.n.01')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_hypernym_d1 = d1.hypernyms()[0]\n",
    "common_hypernym_d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_hypernym_d1.shortest_path_distance(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('mistake.n.01')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_hypernym_d2 = d2.hypernyms()[0]\n",
    "common_hypernym_d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_hypernym_d2.shortest_path_distance(d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Be care comapring verbs, as many verbs don't share common hypernyms. Return value would be None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# read nltk for word collocation\n",
    "\n",
    "- http://www.nltk.org/howto/collocations.html\n",
    "- https://www.nltk.org/\n",
    "\n",
    "- https://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# NLP Resources\n",
    "https://web.stanford.edu/~jurafsky/slp3/\n",
    "\n",
    "http://web.stanford.edu/class/cs224n/\n",
    "\n",
    "1 - Ruder website: http://ruder.io/ (all his tutorials are amazing, I suggest you to start from old posts he has on the website)\n",
    "\n",
    "2 - https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8416973&tag=1 (this covers the most recent advances in DL in NLP)\n",
    "\n",
    "3 - pretty much everything in this website: https://machinelearningmastery.com/category/natural-language-processing/\n",
    "\n",
    "4 - This github repo has a lot of good resources: https://github.com/keon/awesome-nlp\n",
    "\n",
    "5- https://www.youtube.com/watch?v=jfwqRMdTmLo&list=\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
