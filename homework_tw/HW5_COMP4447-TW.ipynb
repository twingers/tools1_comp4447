{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scrapping for Data Scientist job in CO(Total points 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we'll do web scrapping for **Data Scientist job in CO**\n",
    "\n",
    "\n",
    "Here is the link to the search query\n",
    "\n",
    "https://www.indeed.com/jobs?q=data+scientist&l=CO\n",
    "\n",
    "As you can see at the bottom of the page there are link to series of pages related to this search.\n",
    "If you click on second page, search url changes to\n",
    "\n",
    "https://www.indeed.com/jobs?q=data+scientist&l=CO&start=10\n",
    "\n",
    "If you click on 3rd then url changes to\n",
    "\n",
    "https://www.indeed.com/jobs?q=data+scientist&l=CO&start=20\n",
    "\n",
    "Hence to go to more pages we can format search string(**change start=??** part) for **requests.get in a loop**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1(5 =  4(non indicator columns) + 1(indicator columns) points) Please complete the following task\n",
    "\n",
    "- Scrape 10 pages (**last page(10 th) url will be https://www.indeed.com/jobs?q=data+scientist&l=CO&start=90**)and build a pandas DataFrame containing following information\n",
    "    + **job title, name of the company, location, summary of job description**\n",
    "    + **Indicator columns(with value True/False) about keywords Python, SQL, AWS, RESTFUL, Machine learning, Deep Learning, Text Mining, NLP, SAS, Tableau, Sagemaker, TensorFlow, Spark**\n",
    "\n",
    "Note:\n",
    "- Make sure that you do a case insensitive search for keywords when filing(Tue/False) in the indicator columns\n",
    "- You need to go to the webpage of detail job posting for keywords search. Main job posting only contains summary of the job description.  Build detail job posting webpage url  from web scrapping main search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import lxml\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(page, header):\n",
    "    \"\"\"This function scrapes indeed.com to find data scientist jobs in colorado.  The program takes in the number\n",
    "    of pages to scrape and returns soup html code. \"\"\"\n",
    "    \n",
    "    \n",
    "             \n",
    "    url=f'https://www.indeed.com/jobs?q=data%20scientist&l=CO&start={page}'\n",
    "    \n",
    "    req = requests.get(url, headers=header)\n",
    "    print(\"status of request\", req.status_code, \"for page\", page)\n",
    "    soup=bs4.BeautifulSoup(req.text,\"html.parser\")\n",
    "    time.sleep(5)\n",
    "    #print(soup.prettify()) #unhash to see soup code\n",
    "    \n",
    "    return soup\n",
    "      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def url_clean_up(soup):  \n",
    "    \"\"\"This function takes in html text cleaned in beautifulsoup and returns a tuple that contains two lists.  The \n",
    "    first list is URLs for the full job posting and the second is a list of the raw href associated with the full job\n",
    "    postings.  \n",
    "    \n",
    "    Note, there are 15 job postings on each main page.  This function does not include the ad job postings since they\n",
    "    may repeat themselves on each indexed page.  Therefore the function does not always return 15 job posting.\"\"\"\n",
    "    \n",
    "    href_list = []\n",
    "    new_url_list =[]\n",
    "    base_url = \"https://www.indeed.com\"\n",
    "\n",
    "    for job in soup.find_all('a'): #look for all \"a\" with \"href\" attributes\n",
    "        if job.has_attr('href'):\n",
    "            ref = job['href']\n",
    "            new_url = base_url + ref\n",
    "            #print(new_url) # unhash to see all the urls that were created\n",
    "            \n",
    "            \n",
    "            #search for '/rc/clk' and '/company' since they are the in the urls for the full job postings.\n",
    "            # .find() returns -1 if something is not found.  Use double negative to find needed values.\n",
    "            if new_url.find('/rc/clk') != -1: \n",
    "                href_list.append(ref)\n",
    "                new_url_list.append(new_url)\n",
    "            if new_url.find('/company/') != -1:\n",
    "                href_list.append(ref)\n",
    "                new_url_list.append(new_url)    \n",
    "        \n",
    "\n",
    "    return new_url_list, href_list \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_extractor(href, soup):\n",
    "    \"\"\"The function takes in the href list and the full soup from the origional search page.  It uses the href \n",
    "    to look for the location.  The location is then cleaned up to only show the city of the job posting.  The function\n",
    "    returns a list of the cities for the job description\"\"\"\n",
    "    \n",
    "    city = []\n",
    "    for i in range(len(href)):\n",
    "        \n",
    "        loc = soup.find('a', {'href':href[i]}).find('div', {'class':'companyLocation'}).text.lower().strip()\n",
    "        \n",
    "        # convert location to remote if remote is in the location text.  \n",
    "        if 'remote' in loc:\n",
    "            loc = \"remote\"\n",
    "        \n",
    "        #remove everything in the parentheses that contain specific inner city location\n",
    "        loc = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", loc).strip()\n",
    "        \n",
    "        #remove the state\n",
    "        loc = loc.partition(',')[0]\n",
    "        \n",
    "        #append to list and return list\n",
    "        city.append(loc)\n",
    "        \n",
    "    return city\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_df_builder(url, city, header):\n",
    "    \n",
    "    #create the dataframe\n",
    "    job_df = pd.DataFrame(columns=[\"Title\", \"Company\", \"Location\", \"Summary\", \"Python\", \"SQL\", \"AWS\", \"RESTFUL\", \n",
    "                                   \"Machine_Learning\", \"Deep_Learning\", \"Text_Mining\", \"NLP\", \"SAS\", \n",
    "                                   \"Tableau\", \"Sagemaker\", \"TensorFlow\", \"Spark\"])\n",
    "    \n",
    "    #obtain the html from each full job posting\n",
    "    for i in range(len(url)):\n",
    "        req = requests.get(url[i], headers=header)\n",
    "        soup = bs4.BeautifulSoup(req.text, \"html.parser\")\n",
    "        time.sleep(5)\n",
    "        \n",
    "        #parse the soup for the needed data\n",
    "        try:\n",
    "            title = soup.find('h1', {'class':'icl-u-xs-mb--xs icl-u-xs-mt--none jobsearch-JobInfoHeader-title'}).text\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            company = soup.find('div', {'class':'icl-u-lg-mr--sm icl-u-xs-mr--xs'}).text\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            location = city[i] \n",
    "        except:\n",
    "            location = \"nada\"\n",
    "            print(url, \"NO location\")\n",
    "            \n",
    "        try:\n",
    "            summary = soup.find('div', {'class':'jobsearch-jobDescriptionText'}).text.strip()   \n",
    "        except:\n",
    "            continue  \n",
    "            \n",
    "        #print(url[i]) #show weblinks to full job descriptions\n",
    "        \n",
    "        #boolean portion of the dataframe\n",
    "        python = \"python\" in summary.lower()   \n",
    "        sql = 'sql' in summary.lower()\n",
    "        aws = 'aws' in summary.lower()\n",
    "        restful = 'restful' in summary.lower()\n",
    "        machine = ('machine learning' or 'ml') in summary.lower()\n",
    "        deep = \"deep learning\" in summary.lower()\n",
    "        mining = \"text mining\" in summary.lower()\n",
    "        nlp = ('nlp' or 'natural language processing') in summary.lower()\n",
    "        sas = \"SAS\" in summary\n",
    "        tableau = \"tableau\" in summary.lower()\n",
    "        sage = \"sagemaker\" in summary.lower()\n",
    "        tensor = (\"tensorflow\" or \"tensor flow\") in summary.lower()\n",
    "        spark = \"spark\" in summary.lower()\n",
    "            \n",
    "        #appending data to dataframe   \n",
    "        job_df = job_df.append({\"Title\": title, \n",
    "                                \"Company\":company, \n",
    "                                \"Location\":location, \n",
    "                                \"Summary\":summary,\n",
    "                                \"Python\":python, \n",
    "                                \"SQL\":sql, \"AWS\":aws, \n",
    "                                \"RESTFUL\":restful, \n",
    "                                \"Machine_Learning\":machine, \n",
    "                                \"Deep_Learning\":deep, \n",
    "                                \"Text_Mining\":mining, \n",
    "                                \"NLP\":nlp, \n",
    "                                \"SAS\":sas, \n",
    "                                \"Tableau\":tableau, \n",
    "                                \"Sagemaker\":sage, \n",
    "                                \"TensorFlow\":tensor, \n",
    "                                \"Spark\":spark}, ignore_index = True)\n",
    "        \n",
    "    print(job_df)\n",
    "    \n",
    "    return job_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('remote',\n",
       " Python              126\n",
       " Machine_Learning     84\n",
       " SQL                  82\n",
       " Spark                41\n",
       " AWS                  28\n",
       " Tableau              25\n",
       " NLP                  16\n",
       " Deep_Learning        14\n",
       " Text_Mining          10\n",
       " Sagemaker             9\n",
       " TensorFlow            9\n",
       " SAS                   2\n",
       " RESTFUL               0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def question_3a_3b(input_df):\n",
    "    \"\"\"Code for questions 3a and 3b.  results are in the main() function output.\"\"\"\n",
    "    # Quesiton 3a code\n",
    "    q3a = input_df['Location'].max(axis=0)\n",
    "    \n",
    "    # Question 3b code\n",
    "    q3b = (input_df[[\"Python\", \"SQL\", \"AWS\", \"RESTFUL\", \n",
    "                     \"Machine_Learning\", \"Deep_Learning\", \n",
    "                     \"Text_Mining\", \"NLP\", \"SAS\", \"Tableau\", \n",
    "                     \"Sagemaker\", \"TensorFlow\", \"Spark\"]]==True).sum().sort_values(ascending=False)\n",
    "    \n",
    "    return q3a, q3b\n",
    "    \n",
    "    \n",
    "#question_3a_3b(final_df) #run block in its own after main() has been run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Python</th>\n",
       "      <th>SQL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Xcel Energy</td>\n",
       "      <td>denver</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Ent Credit Union</td>\n",
       "      <td>colorado springs</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Associate Decision Scientist, Data &amp; Media</td>\n",
       "      <td>Ibotta</td>\n",
       "      <td>remote</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist - 100% Remote</td>\n",
       "      <td>Frontdoor</td>\n",
       "      <td>remote</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Panasonic Corporation of North America</td>\n",
       "      <td>denver</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Data Scientist II, Client Analytics</td>\n",
       "      <td>Teladoc Health</td>\n",
       "      <td>remote</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Data Scientist - Denver</td>\n",
       "      <td>Dataiku</td>\n",
       "      <td>denver</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Oracle</td>\n",
       "      <td>broomfield</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Data Scientist, Senior</td>\n",
       "      <td>FlightSafety International</td>\n",
       "      <td>denver</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Data Scientist, Distill</td>\n",
       "      <td>TIFIN</td>\n",
       "      <td>boulder</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Title  \\\n",
       "0                                Data Scientist   \n",
       "1                                Data Scientist   \n",
       "5    Associate Decision Scientist, Data & Media   \n",
       "8                  Data Scientist - 100% Remote   \n",
       "9                         Senior Data Scientist   \n",
       "..                                          ...   \n",
       "136         Data Scientist II, Client Analytics   \n",
       "137                     Data Scientist - Denver   \n",
       "138                       Senior Data Scientist   \n",
       "143                      Data Scientist, Senior   \n",
       "144                     Data Scientist, Distill   \n",
       "\n",
       "                                    Company          Location Python   SQL  \n",
       "0                               Xcel Energy            denver   True  True  \n",
       "1                          Ent Credit Union  colorado springs   True  True  \n",
       "5                                    Ibotta            remote   True  True  \n",
       "8                                 Frontdoor            remote   True  True  \n",
       "9    Panasonic Corporation of North America            denver   True  True  \n",
       "..                                      ...               ...    ...   ...  \n",
       "136                          Teladoc Health            remote   True  True  \n",
       "137                                 Dataiku            denver   True  True  \n",
       "138                                  Oracle        broomfield   True  True  \n",
       "143              FlightSafety International            denver   True  True  \n",
       "144                                   TIFIN           boulder   True  True  \n",
       "\n",
       "[81 rows x 5 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def question_3c(input_df):\n",
    "    \"\"\"Code for questions 3c.  results are in the main() function output.\"\"\" \n",
    "    \n",
    "    filtered_df = input_df.filter(items=['Title', 'Company','Location','Python', 'SQL'])\n",
    "    #print(filtered_df)\n",
    "    \n",
    "    q3c=filtered_df.query('Python == True & SQL == True')\n",
    "    \n",
    "    return q3c\n",
    "    \n",
    "    \n",
    "#question_3c(final_df)  #run block in its own after main program has been run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please be patient, this program takes a long time to compile!\n",
      "status of request 200 for page 0\n",
      "status of request 200 for page 1\n",
      "status of request 200 for page 2\n",
      "status of request 200 for page 3\n",
      "status of request 200 for page 4\n",
      "status of request 200 for page 5\n",
      "status of request 200 for page 6\n",
      "status of request 200 for page 7\n",
      "status of request 200 for page 8\n",
      "status of request 200 for page 9\n",
      "URL count 147 city count 147\n",
      "                                                 Title  \\\n",
      "0                                       Data Scientist   \n",
      "1                                       Data Scientist   \n",
      "2                                       Data Scientist   \n",
      "3           Associate Decision Scientist, Data & Media   \n",
      "4                                       Data Scientist   \n",
      "..                                                 ...   \n",
      "142                            Data Scientist - Denver   \n",
      "143  Data Scientist / Artificial Intelligence Resea...   \n",
      "144                                     Data Scientist   \n",
      "145                                     Statistician I   \n",
      "146                             Data Scientist, Senior   \n",
      "\n",
      "                           Company           Location  \\\n",
      "0                      Xcel Energy             denver   \n",
      "1                 Ent Credit Union   colorado springs   \n",
      "2                  YES Communities  greenwood village   \n",
      "3                           Ibotta             remote   \n",
      "4                      VIZIO, Inc.             denver   \n",
      "..                             ...                ...   \n",
      "142                        Dataiku             denver   \n",
      "143  Raytheon Intelligence & Space             aurora   \n",
      "144                     S&P Global         centennial   \n",
      "145              Jackson Lewis P.C             denver   \n",
      "146     FlightSafety International             denver   \n",
      "\n",
      "                                               Summary Python    SQL    AWS  \\\n",
      "0    Responsible for modeling complex business prob...   True   True  False   \n",
      "1    Ent is seeking an experienced, self-directed, ...   True   True   True   \n",
      "2    About YES\\nYES Communities, founded in 2008, o...  False  False  False   \n",
      "3    Ibotta is looking for an Associate Decision Sc...   True   True  False   \n",
      "4    About the Team:\\n\\nVIZIO advertising technolog...   True  False  False   \n",
      "..                                                 ...    ...    ...    ...   \n",
      "142  Headquartered in New York City, Dataiku was fo...   True   True   True   \n",
      "143  The Raytheon Intelligence and Space (RI&S) Dat...   True  False  False   \n",
      "144  The Role: Data Scientist\\n\\nThe Location: Cent...   True   True  False   \n",
      "145  Focused on labor and employment law since 1958...  False  False  False   \n",
      "146  Requisition 28774\\nLocation: Denver, CO Denver...   True   True  False   \n",
      "\n",
      "    RESTFUL Machine_Learning Deep_Learning Text_Mining    NLP    SAS Tableau  \\\n",
      "0     False             True         False        True  False  False   False   \n",
      "1     False             True         False       False  False   True    True   \n",
      "2     False            False         False       False  False  False   False   \n",
      "3     False            False         False       False  False  False   False   \n",
      "4     False             True         False       False  False  False   False   \n",
      "..      ...              ...           ...         ...    ...    ...     ...   \n",
      "142   False             True         False       False  False  False   False   \n",
      "143   False             True         False       False  False  False   False   \n",
      "144   False             True         False       False   True  False   False   \n",
      "145   False            False         False       False  False  False   False   \n",
      "146   False            False         False       False  False  False   False   \n",
      "\n",
      "    Sagemaker TensorFlow  Spark  \n",
      "0       False      False  False  \n",
      "1       False      False   True  \n",
      "2       False      False  False  \n",
      "3       False      False   True  \n",
      "4       False      False   True  \n",
      "..        ...        ...    ...  \n",
      "142     False      False   True  \n",
      "143     False      False  False  \n",
      "144     False      False  False  \n",
      "145     False      False  False  \n",
      "146     False      False  False  \n",
      "\n",
      "[147 rows x 17 columns]\n",
      "\n",
      "\n",
      "Question 3a: most commmon location:   remote\n",
      "\n",
      "\n",
      "Question 3b: output for order of most demandind skill\n",
      " Python              132\n",
      "SQL                  95\n",
      "Machine_Learning     87\n",
      "Tableau              36\n",
      "Spark                31\n",
      "AWS                  29\n",
      "NLP                  19\n",
      "Deep_Learning        10\n",
      "Text_Mining          10\n",
      "SAS                   3\n",
      "Sagemaker             1\n",
      "TensorFlow            1\n",
      "RESTFUL               0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Question 3c: output for Title, Company, and Location for jobs that require Python and SQL.\n",
      "                                           Title                     Company  \\\n",
      "0                                Data Scientist                 Xcel Energy   \n",
      "1                                Data Scientist            Ent Credit Union   \n",
      "3    Associate Decision Scientist, Data & Media                      Ibotta   \n",
      "7                                Data Scientist                    Snapdocs   \n",
      "10                 Data Scientist - 100% Remote                   Frontdoor   \n",
      "..                                          ...                         ...   \n",
      "140                     Data Scientist, Distill                       TIFIN   \n",
      "141                       Senior Data Scientist                      Oracle   \n",
      "142                     Data Scientist - Denver                     Dataiku   \n",
      "144                              Data Scientist                  S&P Global   \n",
      "146                      Data Scientist, Senior  FlightSafety International   \n",
      "\n",
      "             Location Python   SQL  \n",
      "0              denver   True  True  \n",
      "1    colorado springs   True  True  \n",
      "3              remote   True  True  \n",
      "7              denver   True  True  \n",
      "10             remote   True  True  \n",
      "..                ...    ...   ...  \n",
      "140           boulder   True  True  \n",
      "141        broomfield   True  True  \n",
      "142            denver   True  True  \n",
      "144        centennial   True  True  \n",
      "146            denver   True  True  \n",
      "\n",
      "[94 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36'}\n",
    "\n",
    "    full_url_list = []\n",
    "    full_city_list = []\n",
    "    page_count = 0\n",
    "\n",
    "    print(\"Please be patient, this program takes a long time to compile!\")\n",
    "\n",
    "    while page_count < 10: \n",
    "        full_soup = extract(page_count, headers) #initial extraction\n",
    "        #creates a list of urls to active job pages and a list of hrefs\n",
    "        url_list, href_list = url_clean_up(full_soup)  \n",
    "        city_list = location_extractor(href_list, full_soup) #uses hrefs to create a list of cities (cleaned)\n",
    "\n",
    "        #full list created by while-loop iterations \n",
    "        full_url_list = full_url_list + url_list \n",
    "        full_city_list = full_city_list + city_list\n",
    "\n",
    "        page_count += 1\n",
    "\n",
    "    print(\"URL count\", len(full_url_list), \"city count\", len(full_city_list)) # status update of while loop\n",
    "\n",
    "    final_df = job_df_builder(full_url_list, full_city_list, headers) # creates dataframe\n",
    "\n",
    "\n",
    "\n",
    "    final_df.to_pickle(\"indeed_job_co_tw.pkl\")  # this is for question 2\n",
    "    q3a, q3b = question_3a_3b(final_df) #this is for question 3a and 3b\n",
    "    q3c = question_3c(final_df) # this is for question 3c\n",
    "    print(\"\\n\\nQuestion 3a: most commmon location:  \", q3a)\n",
    "    print(\"\\n\\nQuestion 3b: output for order of most demandind skill\\n\", q3b)\n",
    "    print(\"\\n\\nQuestion 3c: output for Title, Company, and Location for jobs that require Python and SQL.\\n\", q3c)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2(1 point) Save you DataFrame to pickle file name *indeed_job_co.pkl*. \n",
    "   Load this pkl file in dataFrame and use this dataFrame for answering following questions.\n",
    "\n",
    "   <font color='red'>upload the pickle file(indeed_job_co.pkl) along with solution notebook to the canvas</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for the pickle file is in the main function.  The code that I used is:\n",
    "\n",
    "final_df.to_pickle(\"indeed_job_co_tw.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"6\" color='red'> Use pandas functionality to answer question 2</font>\n",
    "# Q 3 a(1 point) Which city has maximum job posting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the function, question_3a_3b, above.  The most common location was remote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 3 b(1.5 point) - Top 3 most demanding skills(like Python, AWS, SQL ...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the function, question_3a_3b, above. Python, Machine Learning, SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 c(.5 point) What other questions you would like to ask  based on indeed data?\n",
    "\n",
    "This is free response questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question is what jobs (Title, Company, Location) require both Python SQL? \n",
    "\n",
    "see function question_3c for code and the answser is in the main() function output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
